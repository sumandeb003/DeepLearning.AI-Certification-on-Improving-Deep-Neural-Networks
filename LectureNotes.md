1.  In the previous era of machine learning, it was common practice to take all your data and split it according to maybe a 70/30%....people often talk about the 70/30 train test splits. If you don't have an explicit dev set or maybe a 60/20/20% split, in terms of 60% train, 20% dev and 20% test. And several years ago, this was widely considered best practice in machine learning. If you have here maybe 100 examples in total, maybe 1000 examples in total, maybe after 10,000 examples, these sorts of ratios were perfectly reasonable rules of thumb. But in the modern big data era, where, for example, you might have a million examples in total, then the trend is that your dev and test sets have been becoming a much smaller percentage of the total. Because remember, the goal of the dev set or the development set is that you're going to test different algorithms on it and see which algorithm works better. So the dev set just needs to be big enough for you to evaluate, say, two different algorithm choices or ten different algorithm choices and quickly decide which one is doing better. And you might not need a whole 20% of your data for that. So, for example, if you have a million training examples, you might decide that just having 10,000 examples in your dev set is more than enough to evaluate, you know, which one or two algorithms does better. And in a similar vein, the main goal of your test set is, given your final classifier, to give you a pretty confident estimate of how well it's doing. And again, if you have a million examples, maybe you might decide that 10,000 examples is more than enough in order to evaluate a single classifier and give you a good estimate of how well it's doing. So, in this example, where you have a million examples, if you need just 10,000 for your dev and 10,000 for your test, your ratio will be more like...this 10,000 is 1% of 1 million, so you'll have 98% train, 1% dev, 1% test. And I've also seen applications where, if you have even more than a million examples, you might end up with, you know, 99.5% train and 0.25% dev, 0.25% test. Or maybe a 0.4% dev, 0.1% test.

2. **Looking at your training set error, you can get a sense of how well you are fitting, at least the training data, and so that tells you if you have a $\color{red}{\textrm{bias problem}}$.**
3. **And then looking at how much higher your error goes, when you go from the training set to the dev set, that should give you a sense of how bad is the variance problem.** So you're doing a good job generalizing from a training set to the dev set, that gives you a sense of your variance. All this is under the assumption that the Bayes error is quite small and that your training and your dev sets are drawn from the same distribution.
4. **High variance => Big change in accuracy/error from training set to test/validation set => Overfitting some, not all, data samples by being extra or over flexible in some region of space=> Low error on training set, High error on validation set**
5. **High Bias => Low Accuracy/High Error on training set => Underfitting the data by being linear (instead of being curve, say, quadratic etc)**
6. **Overfitting data in some region while underfitting in some other region of space => High Bias, High Variance**
